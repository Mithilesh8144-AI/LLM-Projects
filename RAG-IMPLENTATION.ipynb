{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66089bb8-4283-47a9-b537-ce8c7289436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e124421-15ad-4751-aebc-e4d9ca0e58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89674ef9-b68a-4cc9-9f2c-f49c2a9a18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca28db7-1a17-4d68-a7f8-f76fd28c8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74727838-3308-4bd3-92f8-729d7b7ee472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36c298c-6ab9-4a05-9a81-cedc2592365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23790683-835d-4edb-a29d-9b4e24413ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def fetch_and_parse(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the webpage content at `url` and return a cleaned string of text.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the webpage to fetch.\n",
    "\n",
    "    Returns:\n",
    "    - str: Cleaned text content extracted from the webpage.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "        irrelevant.decompose()\n",
    "    text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "def split_text_into_documents(text: str, chunk_size: int = 1000, overlap: int = 100):\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks and return them as a list of Documents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The long text to split.\n",
    "    - chunk_size (int): The size of each chunk (default is 1000 characters).\n",
    "    - overlap (int): The number of overlapping characters between consecutive chunks (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of Documents, each containing a chunk of text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the chunks.\n",
    "    docs = []\n",
    "\n",
    "    doc = [Document(page_content = text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3934a216-b3ed-4daf-9ea8-f10910fa843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = fetch_and_parse(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
    "docs = split_text_into_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c87782f-7c16-4360-92b0-0e3474aa102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_stats(texts):\n",
    "    \"\"\"\n",
    "    Calculate and display average word and character statistics for a list of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): A list of Document objects, where each Document contains a `page_content` attribute.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the average word and character counts per document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize variables to keep track of total words and total characters.\n",
    "    total_words, total_characters = 0, 0\n",
    "    total_docs = 0\n",
    "\n",
    "    # Step 2: Iterate through each document in the `texts` list.\n",
    "    for doc in texts:\n",
    "      text = doc.page_content\n",
    "      if text.strip():\n",
    "        total_words += len(text.split())\n",
    "        total_characters += len(text)\n",
    "        total_docs += 1\n",
    "\n",
    "    # Step 3: Calculate the average words and characters per document.\n",
    "    # - Avoid division by zero by checking if the `texts` list is not empty.\n",
    "    if texts:\n",
    "        avg_words = total_words / total_docs\n",
    "        avg_characters = total_characters / total_docs\n",
    "    else:\n",
    "        avg_words = 0\n",
    "        avg_characters = 0\n",
    "    # Step 4: Print the calculated averages in a readable format.\n",
    "    # Example: \"Average words per document: 123.45\"\n",
    "    print(f\"Average words per document: {avg_words}\")\n",
    "    print(f\"Average characters per document: {avg_characters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "054cdf23-d927-4e7d-880c-55bef719d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per document: 155.04032258064515\n",
      "Average characters per document: 956.9153225806451\n"
     ]
    }
   ],
   "source": [
    "calculate_word_stats(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bc9390e-d697-4498-854e-1574754f4d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bc7ec6-8077-4009-ae47-c467e8367b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    A class to implement BM25-based document retrieval.\n",
    "\n",
    "    Attributes:\n",
    "    - documents (list): A list of Document objects.\n",
    "    - corpus (list): A list of strings representing the document contents.\n",
    "    - tokenized_corpus (list): A list of tokenized documents (lists of words).\n",
    "    - bm25 (BM25Okapi): The BM25 retriever initialized with the tokenized corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initialize the BM25 retriever with the given documents.\n",
    "\n",
    "        Parameters:\n",
    "        - documents (list): A list of Document objects.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.corpus = [doc.page_content for doc in documents]\n",
    "\n",
    "        self.tokenized_corpus = [doc.split() for doc in self.corpus]\n",
    "\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve the top `k` most relevant documents for a given query.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query as a string.\n",
    "        - k (int): The number of top documents to return (default is 5).\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of the top `k` relevant documents as strings.\n",
    "        \"\"\"\n",
    "\n",
    "        tokenized_query = query.split()\n",
    "\n",
    "        top_docs = self.bm25.get_top_n(tokenized_query, self.corpus, n=k)\n",
    "\n",
    "        return top_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a98b9b4-a60a-472b-954b-09b618c9489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_chroma(documents: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    Build a Chroma vector store using Hugging Face embeddings\n",
    "    and add the documents to it.\n",
    "\n",
    "    Parameters:\n",
    "    - documents (list[Document]): A list of Document objects to add to the vector store.\n",
    "\n",
    "    Returns:\n",
    "    - Chroma: The Chroma vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "    vector_store.add_documents(documents)\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93365d96-ff7d-42db-973e-5179d725c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "class EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Merges results from Chroma similarity search and BM25 lexical search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chroma_store, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleRetriever with Chroma and BM25 retrievers.\n",
    "\n",
    "        Parameters:\n",
    "        - chroma_store: The Chroma vector store for semantic retrieval.\n",
    "        - bm25_retriever: The BM25 retriever for lexical retrieval.\n",
    "        \"\"\"\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.chroma_store = chroma_store\n",
    "\n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents by combining results from Chroma and BM25.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input search query.\n",
    "        - k (int): The number of top unique documents to return (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - list[Document]: A list of unique relevant documents.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        chroma_docs = self.chroma_store.similarity_search(query, k=k)\n",
    "     \n",
    "        bm25_docs = self.bm25_retriever.retrieve(query, k=k)\n",
    "\n",
    "        combined = chroma_docs + [Document(page_content=doc) for doc in bm25_docs]\n",
    "     \n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in combined:\n",
    "\n",
    "            content = doc.page_content if isinstance(doc, Document) else doc\n",
    "      \n",
    "            key = content[:60]\n",
    "\n",
    "            if key not in seen:\n",
    "                if isinstance(doc, str):\n",
    "                    doc = Document(page_content=doc)\n",
    "                unique_docs.append(doc)\n",
    "                seen.add(key)\n",
    "\n",
    "        return unique_docs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e76b7dd-9619-4bef-8b05-f61ffe1dd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "class EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Merges results from Chroma similarity search and BM25 lexical search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chroma_store, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleRetriever with Chroma and BM25 retrievers.\n",
    "\n",
    "        Parameters:\n",
    "        - chroma_store: The Chroma vector store for semantic retrieval.\n",
    "        - bm25_retriever: The BM25 retriever for lexical retrieval.\n",
    "        \"\"\"\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.chroma_store = chroma_store\n",
    "\n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents by combining results from Chroma and BM25.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input search query.\n",
    "        - k (int): The number of top unique documents to return (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - list[Document]: A list of unique relevant documents.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        chroma_docs = self.chroma_store.similarity_search(query, k=k)\n",
    "     \n",
    "        bm25_docs = self.bm25_retriever.retrieve(query, k=k)\n",
    "\n",
    "        combined = chroma_docs + [Document(page_content=doc) for doc in bm25_docs]\n",
    "     \n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in combined:\n",
    "\n",
    "            content = doc.page_content if isinstance(doc, Document) else doc\n",
    "      \n",
    "            key = content[:60]\n",
    "\n",
    "            if key not in seen:\n",
    "                if isinstance(doc, str):\n",
    "                    doc = Document(page_content=doc)\n",
    "                unique_docs.append(doc)\n",
    "                seen.add(key)\n",
    "\n",
    "        return unique_docs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71ccc495-9bb4-4d38-a710-bde9d1c54e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 248 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "# BEFORE\n",
    "# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "\n",
    "# AFTER\n",
    "vectorstore = FAISS.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "total_vectors = vectorstore.index.ntotal\n",
    "dimensions = vectorstore.index.d\n",
    "\n",
    "print(f\"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77f389e9-3688-4653-8ead-4665309d3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6f6dc61-85f6-47b7-82d3-b796fbdede03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is a field of research in computer science that focuses on creating systems and software that enable machines to perceive their environment, learn from data, and take actions to achieve defined goals. It encompasses various approaches, including machine learning, deep learning, symbolic reasoning, and more. AI can be applied in numerous areas, such as natural language processing, computer vision, robotics, and many others. High-profile applications of AI include web search engines, recommendation systems, virtual assistants, autonomous vehicles, and generative tools. The goal of AI research is to develop intelligent agents that can perform tasks that typically require human intelligence.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe AI\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "899c858e-c44e-4278-954f-6d94f229413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0eab4a33-1e22-451e-bff1-900a4a1b0696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Machine_learning\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "example_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    ]\n",
    "\n",
    "    # Step 1A: Initialize an empty list to store all documents\n",
    "\n",
    "\n",
    "# Create the EnsembleRetriever instance\n",
    "ensemble_retriever = EnsembleRetriever(chroma_store=chroma_store, bm25_retriever=bm25_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38b4021b-0bfb-464a-b2ce-104f0dd51cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... other imports ...\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "system_message = \"You are a helpful assistant \"\n",
    "system_message += \"Give proper descriptions as required by the user, explain it in detail\"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\"\n",
    "\n",
    "def get_ai_response(user_prompt, example_urls):\n",
    "    \"\"\"\n",
    "    Gets an AI response to a user prompt, incorporating relevant context.\n",
    "\n",
    "    Parameters:\n",
    "    - user_prompt (str): The input prompt from the user.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the AI's response text and an optional image.\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    \n",
    "    # Step 1B: Iterate through the URLs to fetch and process content\n",
    "    for url in example_urls:\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "    \n",
    "    # Step 1B.1: Fetch and parse the raw text from the URL\n",
    "        raw_text = fetch_and_parse(url)\n",
    "    \n",
    "    # Step 1B.2: Split the raw text into chunks (documents)\n",
    "        splits = split_text_into_documents(raw_text)\n",
    "    \n",
    "    # Step 1B.3: Add the chunks to the list of documents\n",
    "        all_docs.extend(splits)\n",
    "    \n",
    "    chroma_store = build_chroma(all_docs)\n",
    "\n",
    "    # Step 2B: Build the BM25 retriever\n",
    "    bm25_retriever = BM25Retriever(all_docs)\n",
    "        \n",
    "    ensemble_retriever = EnsembleRetriever(chroma_store=chroma_store, bm25_retriever=bm25_retriever)\n",
    "\n",
    "    \n",
    "    relevant_docs = ensemble_retriever.get_relevant_documents(user_prompt, k=3)\n",
    "    context = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nUser's prompt: {user_prompt}\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "\n",
    "    \n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee2354d6-64c1-4a36-b39a-f192c0ed2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping content from: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html\n"
     ]
    }
   ],
   "source": [
    "def gradio_interface(urls, user_prompt):\n",
    "    urls_list = urls.split(\",\")  # Assuming the input URLs are comma-separated\n",
    "    response = get_ai_response(user_prompt, urls_list)\n",
    "    return response\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter URLs (comma-separated)\"),\n",
    "        gr.Textbox(label=\"User Prompt\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"AI Assistant with RAG handing forURL Context and BM25 Retriever + Chroma store\",\n",
    "    description=\"Enter a list of URLs to retrieve context, along with your prompt.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3fefd-d73a-46cd-a52d-60d05020b8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
